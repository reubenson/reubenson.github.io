This project will be entirely web-based and optimized for mobile devices (though laptops and desktop computers will not be excluded). The project can either be self-hosted, or hosted on a domain provided by websoundart.org. It will consist of open-source JavaScript/TypeScript, CSS, and HTML, and may contain additional dependencies to third-party libraries such as [meyda.js](https://meyda.js.org/). Depending on the deployment architecture, it will likely be a single-page application built with a simple reactive framework such as Svelte (and not a heavier framework, such as React). GitHub and git versioning will be used to share code and updates as the project progresses, and documentation for the project will be written to allow audiences to imagine building projects themselves.

This project relies entirely on the Web Audio API built into standard web browsers, which will provide the basis of how each device listens to its environment and makes frog sounds. The basic operation of the app is to apply real-time audio analysis operations to the device's microphone input, and determine whether frog sounds are being heard, or not.

<figure>
  <img src="https://reubenson-portfolio.s3.us-east-1.amazonaws.com/assets/hess_diagram.jpeg" alt="flow diagram for Felix Hess's frog behavior">
  <figcaption>Felix Hess's original flow diagram of robot-frog behavior; source: <a href="https://isea-archives.siggraph.org/art-events/electronic-sound-creatures-by-felix-hess/" target="_blank">Electronic Sound Creatures</a></figcaption>
</figure>

I've used the Web Audio API before to build creative software projects before, like the one powering [Hobo UFO](https://p-a-n.org/release/james-hoff-hobo-ufo-v-chernobyl/), in which camera movements with Google Maps Street View are controlled dynamically by analysizing audio in real-time. I'm confident that the Web Audio API will provide all the functionality required to power this application ([fast fourier transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform#:~:text=A%20fast%20Fourier%20transform%20(FFT,frequency%20domain%20and%20vice%20versa.)) analysis of an incoming audio signals, and playback of pre-defined audio files). The trickiness of this project, however, will be in dialing in the behavior, to be neither too sensitive nor too coarse. It would be cool to use machine learning to improve the capacity to detect frog sounds from the microphone, but its usefulness is not guaranteed and may detract from other critical aspects of the project's development. 

For this reason, I would aim to have the bulk of the programming finished at least halfway through the allotted timeline, so that sufficient time can be devoted to field-testing the app, and if time permits, to explore incorporating machine learning at a later point. I would like to note, however, that QA for this project will always be necessarily incomplete, due to the wide range of devices and environments possible. But even in the event that a group of participants fails to get their devices to sing to each other, the poetic capacity for such an event occuring is sufficient to consider any given performance to be successful. According to Felix himself, his frog-robots were so sensitive that in order for his installations to work, [he would sometimes need to go around turning off every possible thing that was making sound](https://bldgblog.com/2008/04/space-as-a-symphony-of-turning-off-sounds/), such as HVAC systems and other electrical appliances. In this sense, every technical failure is a creative opportunity and poetic action.