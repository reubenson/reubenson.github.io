<p>This project will be entirely web-based and optimized for mobile devices (though laptops and desktop computers will not be excluded). The project can either be self-hosted, or hosted on a domain provided by websoundart.org. It will consist of JavaScript/TypeScript, CSS, and HTML, and may contain additional dependencies to third-party libraries such as <a href="https://meyda.js.org/">meyda.js</a>. Depending on the deployment architecture, it will likely be a single-page application built with a simple reactive framework such as Svelte (and not a heavier framework, such as React). GitHub and git versioning will be used to share code and updates as the project progresses, and documentation for the project will be written to give other artists and developers a foundation to build similar projects.</p>
<p>This project relies entirely on the Web Audio API (supported in all standard web browsers), which will provide the basis of how each device listens to its environment and makes frog sounds (from audio samples). The basic operation of the app is to apply real-time audio analysis operations to the device's microphone input, and determine whether frog sounds are being heard, or not.</p>
<figure>
  <img src="https://reubenson-portfolio.s3.us-east-1.amazonaws.com/assets/hess_diagram.jpeg" alt="flow diagram for Felix Hess's frog behavior">
  <figcaption>Felix Hess's original flow diagram of robot-frog behavior; source: <a href="https://isea-archives.siggraph.org/art-events/electronic-sound-creatures-by-felix-hess/" target="_blank">Electronic Sound Creatures</a></figcaption>
</figure>
<p>I've used the Web Audio API before to build creative software projects before, like the one powering <a href="https://p-a-n.org/release/james-hoff-hobo-ufo-v-chernobyl/">Hobo UFO</a>, in which camera movements within Google Maps Street View are controlled dynamically by analysizing audio in real-time. I'm confident that the Web Audio API will provide all the functionality required to power this application (<a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform#:~:text=A%20fast%20Fourier%20transform%20(FFT,frequency%20domain%20and%20vice%20versa.)">fast fourier transform</a> analysis of an incoming audio signals, and playback of predetermined audio files). The trickiness of this project, however, will be in calibrating the behavior, to be neither too sensitive nor too coarse. It could be interesting to use machine learning to improve the capacity for detecting frog sounds, but its implementation would take time and may detract from other interesting aspects of the project's development. Accessibility and screen-reader usability are also high priorities for this project, and I would like to align this project towards an internet that is more heard than seen/read.</p>
<p>I would aim to have the bulk of the programming finished at least halfway through the allotted timeline, so that sufficient time can be devoted to field-testing the app. I would like to note, however, that QA for this project will always be necessarily incomplete, due to the wide range of devices and environments possible. But even in the event that a group of participants fails to get their devices to sing to each other, the poetic capacity for such an event occuring is sufficient to consider any given performance to be successful. According to Felix himself, his frog-robots were so sensitive that in order for his installations to work, <a href="https://bldgblog.com/2008/04/space-as-a-symphony-of-turning-off-sounds/">he would sometimes need to go around turning off every possible thing that was making sound</a>, such as HVAC systems and other electrical appliances. In this sense, every technical failure is a creative opportunity.</p>
