<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="en">
  <title>Reuben Son | Recurse Center</title>
  <subtitle>Weekly writings from my time at Recurse Center</subtitle>
  <link href="https://reubenson.com/feed.xml" rel="self"/>
  <link href="https://reubenson.com/"/>
  <updated>2023-11-19T15:33:19Z</updated>
  <id>https://reubenson.com/</id>
  <author>
    <name>Reuben Son</name>
    <email>reubenson@gmail.com</email>
  </author>
  
  <entry>
    <title>Alignment Is All You Need</title>
    <link href="https://reubenson.com/recurse/week-3/"/>
    <updated>2023-11-19T15:33:19Z</updated>
    <id>https://reubenson.com/recurse/week-3/</id>
    <content type="html">&lt;h2&gt;Alignment Is All You Need&lt;/h2&gt;
&lt;h3&gt;Week 3 at Recurse Center&lt;/h3&gt;
&lt;p&gt;I&#39;ve just finished my third week at Recurse Center, which means I&#39;m already halfway through my residency here. Although I&#39;m still digesting the material in &lt;a href=&quot;https://karpathy.ai/zero-to-hero.html&quot;&gt;Andrej Karpathy&#39;s Neural Net series&lt;/a&gt;, I&#39;ve finished working through the videos, and in the coming week, I&#39;ll be continuing to implement my own model system to improve my understanding and intuition about these systems.&lt;/p&gt;
&lt;p&gt;The last video in the series is centered around the pivotal &lt;a href=&quot;https://reubenson.com/recurse/week-3/&quot;&gt;Attention is All You Need&lt;/a&gt; paper, which presents the &lt;em&gt;transformer&lt;/em&gt; architecture central to OpenAI&#39;s GPT models. In working through Karpathy&#39;s lecture series, I&#39;m struck by the elegance of neural net architecture, their composition through the repetition of simple elements. And I&#39;m struck too by the degree to which much of the &amp;quot;art&amp;quot; of designing these systems relies on simple arithmetic operations that nudge the state of the system within a regime of being &amp;quot;well-behaved&amp;quot;. Scale is the main thing, which led Rich Sutton to conclude in his &lt;a href=&quot;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&quot;&gt;&lt;em&gt;Bitter Lesson&lt;/em&gt;&lt;/a&gt;: &amp;quot;We have to learn the bitter lesson that building in how we think we think does not work in the long run.&amp;quot;&lt;/p&gt;
&lt;p&gt;Meanwhile ... OpenAI continues to make dramatic headlines, with the sudden firing of Sam Altman. At the time of this writing, conflicts within the board around the speed and safety of the development of their products seem to be at the core of this rift.&lt;/p&gt;
&lt;p&gt;The question of social disalignment around the topic of &lt;em&gt;AI Alignment&lt;/em&gt; within the leadership team at OpenAI is poignant, provocative, and perhaps all too predictable ... disalignment around what it means to build in how we think. Institutions have emergent behaviors and a kind of artificial intelligence too. It seems worth pondering how theories around the problem of alignment in both technologic and social domains emerged at a similar time, as is explored by Orit Halpern in a &lt;a href=&quot;https://www.journals.uchicago.edu/doi/10.1086/717313&quot;&gt;recent paper on the conjoined histories of neural nets and Hayekian neoliberal economics&lt;/a&gt;. Which is to say, &lt;a href=&quot;https://www.reddit.com/r/singularity/comments/17yxl1x/the_head_of_applied_research_at_openai_seems_to/&quot;&gt;e/acc&lt;/a&gt; is nothing new, and alignment has perhaps been out of vogue for nearly a century.&lt;/p&gt;
&lt;p&gt;In my own microcosm of space-time at Recurse, the question of alignment (on a personal scale) feels like a desire for the reassertion of control: in optimizing my time here, having finished projects, the setting and completion of goals. Alignment presumes there&#39;s a reward or loss function at the heart of the model/subject (as is the case for both neural nets and neoliberal economics), but the archetype of the &lt;a href=&quot;https://miguelabreugallery.com/wp-content/uploads/2016/12/GroundingVision_MAG_2017_Sequence_06-1-1024x276.jpg&quot;&gt;meandering line&lt;/a&gt; still feels more appropriate to me. And I wonder, too, about the inverse of Sutton&#39;s &lt;em&gt;Bitter Lesson&lt;/em&gt;, of how the things we build changes how we think, how the logic embedded in formal systems become heuristics for our own sense-making.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Gradually, Then Suddenly</title>
    <link href="https://reubenson.com/recurse/week-2/"/>
    <updated>2023-11-11T18:30:32Z</updated>
    <id>https://reubenson.com/recurse/week-2/</id>
    <content type="html">&lt;h2&gt;Gradually, Then Suddenly&lt;/h2&gt;
&lt;h3&gt;Week 2 at Recurse Center&lt;/h3&gt;
&lt;p&gt;Earlier this week, at an informal dinner centered around the theme of AI Safety, I saw Rob Nail of Singularity University quote &lt;em&gt;&amp;quot;gradually, then suddenly&amp;quot;&lt;/em&gt; (attribution often asigned to Ernest Hemingway, on going bankrupt) in a presentation. It feels equally apt describing the subjective experience of time while at Recurse Center as it does exponential technologies. While I&#39;d been vaguely familiar with AI Safety, this week has gone a long ways towards concretizing my understanding of this emerging field of research and development (thanks in no small part to &lt;a href=&quot;https://github.com/changlinli&quot;&gt;Changlin Li&lt;/a&gt;&#39;s generous efforts to bring me and my cohorts at Recurse up to speed!).&lt;/p&gt;
&lt;p&gt;As of right now, I&#39;ve made it through 4 of the 7 &lt;a href=&quot;https://karpathy.ai/zero-to-hero.html&quot;&gt;videos in Andrej Karpathy&#39;s lecture series on neural nets&lt;/a&gt;, and the first two chapters of the &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;fastbook notebooks&lt;/a&gt;. The majority of my effort is still towards getting up to speed on fundamentals. But that said, it seems worthwhile to think through some of my current questions, assumptions, and intuitions here, if only for my future benefit in tracking how my views shift over time.&lt;/p&gt;
&lt;h3&gt;On AGI&lt;/h3&gt;
&lt;p&gt;AGI (Artificial General Intelligence) seems like a rather squishy term that is dependant on significant, fundamental vagaries: around the nature of our own intelligence, and the question of interpreting/measuring intelligence of an AI system. But it does seem plausible (and probable even) that if we &lt;em&gt;do&lt;/em&gt; build an AGI, we&#39;ll do so before fully understanding how that emergent behavior was achieved. And it will be even longer still, before any meaningful consensus is developed around whether AGI has actually been achieved.&lt;/p&gt;
&lt;p&gt;So given the long arc of developing and recognizing systems that approach AGI, I wonder what it would mean to give space to such systems?&lt;/p&gt;
&lt;!-- In the longer arc of developing systems toward AGI , I wonder what it would mean to give space to such systems --&gt;
&lt;!-- As funding and development of AI outpace AI Safety,  --&gt;
&lt;!-- 
So ... will we reach AGI or what

I have inclinations to debate some of the terminology involved here; ground truth on words that are routinely used to describe current and future technology is fundamentally impossible to pin down. Shortlist of such words: learning, intelligence, artificial, human | machine. And to answer this question is truly beyond me, and I&#39;m happy enough in this case to defer to experts. In some ways, I&#39;m not really interested to address this question at all. But I do wonder what it means to give space to such systems? --&gt;
&lt;p&gt;From what I&#39;ve learned of ML systems thus far, they&#39;re tightly bounded by two constraints: data and computing resources. Model architectures optimize across these two constraints, and the success of current state of the art (LLMs like GPT) relies on transformer-based architecture which rapidly advanced such optimizations. While current transformer-based models may be transient (and there are reasons to hope that they&#39;ll be replaced by architectures that are easier to evaluate according to AI Safety standards), all future architectures will continue to be constrained by data and computing resources on some fundamental level. As these systems scale up in capability, the bottlenecks between subsequent developments will come from doing the most with what data these systems have access to, maximizing the latent knowledge built up from data inflows.&lt;/p&gt;
&lt;!-- As of right now, images and text are the richest sources of data that we have, due to their representational efficiency. --&gt;
&lt;p&gt;Given this, what happens in the limit as we approach AGI? We may currently be in the regime of low-hanging fruit as current state of the art is trained on the internet as its corpus, but richer data-mining would come from greater interaction between models and humans. Heterogeneous systems comprised of humans and AIs interacting would therefore be incentivized from both a training and utility perspective.&lt;/p&gt;
&lt;p&gt;Such heterogeneous systems might perhaps start out being more human-centric. For example, let&#39;s say Apple deploys a new OS that has an AI agent that can automatically answer your emails upon prompting you with a simple question here and there. Much like no one wants an email signature that reads &lt;code&gt;Sent from Reuben&#39;s iPad&lt;/code&gt;, users typically won&#39;t want to send emails that obviously read as being AI-generated. Therefore, in this heterogeneous system, the user will spend time training the AI properly, in order to maintain consistency of tone. Along this path, there will be a tipping point in which it is much easier to give the AI full control over writing one&#39;s emails, than to shut it off and go back to writing your own.&lt;/p&gt;
&lt;p&gt;This in itself doesn&#39;t seem like such a bad thing, but as heterogeneous systems increase in scale, breadth, and interconnectivity, a tendency will exist to make interfaces more and more uniform, to be accessible to both human and AI agents. For example, a new housing development may design an apartment layout that optimizes for human preference and AI inferrability, or a municipalities may decide to revamp its roadways in a way that accomodates driverless cars alongside human drivers.&lt;/p&gt;
&lt;p&gt;The more we increase the surface area of interconnected complex heterogeneous systems (AI augmenting each layer of &lt;a href=&quot;https://thestack.org/&quot;&gt;&lt;em&gt;The Stack&lt;/em&gt; as defined by Benjamin Bratton&lt;/a&gt;), the more difficult it may be to unplug a given set of AI technologies without risking cascading failures across many systems. &lt;a href=&quot;https://ourworldindata.org/ai-timelines&quot;&gt;Many AI researchers seem to be reaching consensus about human-level AI being achieved within 100 years&lt;/a&gt;, but trends towards uniform interfaces will happen long before AGI&#39;s arrival.&lt;/p&gt;
&lt;!-- Such heterogeneous systems might then be characterized in the following ways: --&gt;
&lt;!-- - More uniform interfaces, to be accessible to both humans and AI agents. For example, a new housing development may design an apartment layout that optimizes for human preference and AI inferrability --&gt;
&lt;!-- As a result, I would speculate some of the following to be the case, in the optimization of AGI (along the constraints of compute and data) within the context of increasingly large surface areas of heterogeneous systems inb which humans and actively-training models coexist --&gt;
&lt;!-- - More uniform interfaces (e.g. changing roadway infrastructure to balance the requirements of machine vision and human vision) --&gt;
&lt;!-- - Terminology seems like a fundamentally contentious complex issue when talking about AI. I&#39;m still hesitant to even describe the current technology as AI, as opposed to Machine Learning. At the asymptote of the current development, I&#39;d be more inclined to describe the resulting technology as Machine Intelligence, as opposed to Artificial Intelligence.  --&gt;
&lt;!-- -  --&gt;</content>
  </entry>
  
  <entry>
    <title>Week 1 at Recurse Center (Learning to Paddleboard)</title>
    <link href="https://reubenson.com/recurse/week-1/"/>
    <updated>2023-11-05T17:23:18Z</updated>
    <id>https://reubenson.com/recurse/week-1/</id>
    <content type="html">&lt;h2&gt;Week 1 at Recurse Center (Learning to Paddleboard)&lt;/h2&gt;
&lt;p&gt;Twice this year, I&#39;ve had the good fortune to find myself in spaces filled with people aligned to a common purpose: earlier this year at &lt;a href=&quot;https://www.haystack-mtn.org/&quot;&gt;Haystack Mountain Schoool of Craft&lt;/a&gt;, and now at &lt;a href=&quot;https://www.recurse.com/&quot;&gt;Recurse Center&lt;/a&gt;. These environments foster a regime of expansion and change, and have the power to focus, broaden, and alter one&#39;s sense of purpose. These environments can be quite intense to adjust and recalibrate to, as they involve large changes to the core substrates and infrastructures that compose of one&#39;s daily existence: time, geography, architecture, and community. Much of this is self-evident and not particularly profound to point out, but its lived experience can most immediately be felt in the constellation of affective states that manifest throughout the transition: overhwelm, nervous excitement, disorientation, decision paralysis, fight-or-flight, hyperactivity, dissociation.&lt;/p&gt;
&lt;p&gt;This summer, at Haystack, in addition to learning the &lt;a href=&quot;https://medium.com/@reubenson/foray-into-3d-printing-with-clay-at-haystack-207064511cd&quot;&gt;workflows and processes of ceramic 3D printing&lt;/a&gt;, I went paddleboarding for the very first time. On my initial attempts to stand up and paddle about, I was quickly tossed into the water. The Haystack campus is built on a granite island off the Maine coast, and faces out directly to the ocean. After falling a couple times into the rocking waves, I decided to paddle sitting down instead for a while, but before finally returning to shore, I decided to just stand on the board for a bit, without paddling. I then sat back down again and rowed back to shore. The next day, and each subsequent day on the beach, I found myself immediately able to paddleboard standing up without any further issue. The last time I watched my body in the process of learning was when I began taking ceramics classes in 2019, and it struck me, more or less for the first time, how quickly, and quietly, embodied intelligence is set into motion.&lt;/p&gt;
&lt;p&gt;I think of acclimating to environments like Recurse in a similar way, that the transition still passes through the body, even for something as heady as Recurse&#39;s guiding mission to enable its participants to become radically better programmers over the duration of the residency.&lt;/p&gt;
&lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt;
&lt;p&gt;So, it&#39;s been a lot of just that, riding the waves through varying affective states, while also slowly refining my sense of purpose here. I&#39;ve been working out a set of priorities and goals, in order to make more tangible that sense of purpose, which I&#39;ll share below. It&#39;s a little sobering that after having just wrapped the first week of my six-week residency here, I&#39;m already nearly a quarter of the way through.&lt;/p&gt;
&lt;p&gt;So far, I&#39;ve committed to going through some amount of &lt;a href=&quot;https://karpathy.ai/zero-to-hero.html&quot;&gt;Andrej Karpathy&#39;s introduction to building neural networks from scratch&lt;/a&gt; and Russell Webb&#39;s &lt;a href=&quot;https://github.com/Russ741/karpathy-nn-z2h&quot;&gt;accompanying worksheets&lt;/a&gt;, though I&#39;m also likely going to switch to or ping-pong between that series and the &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;Fast.ai series&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Guiding principles&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Stay open-ended and cultivate sensitivity, paying attention to what drives my own interests in tech, and see what it is that drives other programmers to similar or divergent interests&lt;/li&gt;
&lt;li&gt;Think of the residency as more of an onboarding, not an end to itself&lt;/li&gt;
&lt;li&gt;Embrace being process-oriented, balanced against being results-driven (but not necessarily working backwards from a predetermined outcome)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Goals&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn a new programming language (Python)&lt;/li&gt;
&lt;li&gt;Learn the fundamentals of ML&lt;/li&gt;
&lt;li&gt;Get feedback on previous/existing projects, like &lt;a href=&quot;https://frogchor.us/&quot;&gt;Frog Chorus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finish and share at least one small project&lt;/li&gt;
&lt;li&gt;Understand current employment landscape / opportunities&lt;/li&gt;
&lt;li&gt;Give focus to both writing code and text (like this entry)&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
</feed>