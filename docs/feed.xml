<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:base="en">
  <title>Reuben Son | Recurse Center</title>
  <subtitle>Weekly writings from my time at Recurse Center</subtitle>
  <link href="https://reubenson.com/feed.xml" rel="self"/>
  <link href="https://reubenson.com/"/>
  <updated>2025-05-28T07:46:24Z</updated>
  <id>https://reubenson.com/</id>
  <author>
    <name>Reuben Son</name>
    <email>reubenson@gmail.com</email>
  </author>
  
  <entry>
    <title>Wrapping up</title>
    <link href="https://reubenson.com/recurse/week-6/"/>
    <updated>2025-05-28T07:46:24Z</updated>
    <id>https://reubenson.com/recurse/week-6/</id>
    <content type="html">&lt;h3&gt;Week 6 at Recurse Center&lt;/h3&gt;
&lt;p&gt;It’s been such a fast six weeks at Recurse, and all the more so for getting so absorbed in getting my MIDI Archive project into shape. It’s been a journey … which I think I’ll need some time to sit with before writing out a proper reflection of my wonderful and expansive time here.&lt;/p&gt;
&lt;p&gt;For now, I’m still figuring out a path to an MVP version of this project, which I’m hoping to chip away at the next few weeks as I adjust to a new post-RC equilibrium. I’m also struggling with uncertainty and doubt about the technical and cultural value of this project, and whether it makes sense to put much more effort into an MVP. At worst, it feels unclear to me whether I’m just re-enacting the algorithmic instrumentalization of music (a lá Spotify) in a more amateur way, on a more campy substrate.&lt;/p&gt;
&lt;p&gt;But for the moment, I’ll just limit my scope to documenting and preserving the groundwork I’ve laid out thus far. And while still in very skeletal form, the project has at least been shipped to prod! &lt;a href=&quot;http://reubenson.com/midi-archive/&quot;&gt;http://reubenson.com/midi-archive/&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;THE PROJECT&lt;/h4&gt;
&lt;p&gt;At the core of this project, I’ve been driven by two broad questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What, if anything, does the pre-MP3 history of music on the web say about the present moment? Or its future?&lt;/li&gt;
&lt;li&gt;How the heck does machine learning actually work?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the six weeks of my residency at Recurse, I feel like I’m still just scratching the surface of these questions. But as I’ve heard writers say, you write a story in order to figure out how it ends.&lt;/p&gt;
&lt;p&gt;In my approach to these two questions, I’ve been building a neural net model alongside an archive, in which the model informs my curation of the archive, and the curation of the archive influences how the model behaves. In more straightforward terms, what I’ve been doing is the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explore MIDI websites on the early web, and scraping ones that feel culturally/aesthetically interesting and/or representative. I’ve eschewed using canonical datasets like &lt;a href=&quot;https://magenta.tensorflow.org/datasets/maestro&quot;&gt;Maestro&lt;/a&gt; or large existing archives like the &lt;a href=&quot;https://archive.org/details/archiveteam-geocities-midi-collection-2009&quot;&gt;Geocities MIDI collection&lt;/a&gt;. Thus far, I’ve scraped together ~3000 MIDI files, which is not a lot. In some sense, I’ve been loosely following principles described by Everest Pipkin in their &lt;a href=&quot;https://www.youtube.com/watch?v=IYNKs8vfocc&quot;&gt;“Corpora as Medium” talk&lt;/a&gt; on the importance of curation in working with large language models. Given that, I’ve been taking a more iterative approach to building up the archive.&lt;/li&gt;
&lt;li&gt;Use these MIDI files to train a neural net model built on PyTorch in order to generate new MIDI files. My main goal with this model is to be of educational use, not necessarily trying to make it as sophisticated as possible, so I’ve been pretty happy with the relatively naive results I’ve gotten thus far. I find the results amusing, but I’m probably biased. It reminds me a bit of the main plot-point in &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Fly_(1986_film)&quot;&gt;The Fly&lt;/a&gt;, in which Jeff Goldblum fuses with a housefly …&lt;/li&gt;
&lt;li&gt;The MIDI Archive and the ML output is featured alongside each other on the &lt;a href=&quot;https://reubenson.com/midi-archive&quot;&gt;MIDI Archive website&lt;/a&gt;. I had originally planned on having the ML output presented as a ‘daily broadcast/performance’, but haven’t gotten around to implementing that bit yet. At the moment, the loosely prototyped website allows visitors to see and listen to all the MIDI files that were used for training and validating the model, as well as listen to MIDI music generated (daily) by the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As for what’s next for this project … the main thing that feels lacking at the moment is a sense for what visitors, who may or may not be familiar with music on the early web, should expect to find here. I’m not particularly interested in just exploring the nostalgic valence of this work, but want to dig deeper into why I may have been drawn to this project in the first place …&lt;/p&gt;
&lt;p&gt;But for now, feel free to take a look at the two repositories I’ve spun up for this project, but be warned that documentation and cleanup is still in a messy state&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/reubenson/midi-archive&quot;&gt;https://github.com/reubenson/midi-archive&lt;/a&gt; contains the scraper and 11ty sit generator&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/reubenson/midi-archive-lambda&quot;&gt;https://github.com/reubenson/midi-archive-lambda&lt;/a&gt; contains the lambda functions used for deployed model, and the notebook used to train the model&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  <entry>
    <title>On deploying a neural net to AWS Lambda</title>
    <link href="https://reubenson.com/recurse/week-5/"/>
    <updated>2025-05-28T07:46:24Z</updated>
    <id>https://reubenson.com/recurse/week-5/</id>
    <content type="html">&lt;h3&gt;Week 5 at Recurse Center&lt;/h3&gt;
&lt;p&gt;I’m heading into my last week at Recurse, and feeling an excited rush to wrap up the MIDI archive project I began ruminating on the previous week. The shape of it has been slowly coming together, and what remains to be seen is how much I can manage to deploy in the handful of days I have left. At the moment, the project looks something like the following, involving many components beyond just neural net model.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;https://mermaid.ink/img/pako:eNplUsFqGzEQ_ZVBJwXiQujNh4LttR1DCgUHmqSbw1ia3RXRStuR1ls35N8r7W5JQ3UaoffezLynV6G8JrEUlfWDapAj3Belg3RW8qgYO-IrWCy-PMnCD8561PD1UBygMpYCVOxbOLhI7CjCilVjznQ18Z8yDdbySHymiYQhUAxwNgh7E5v-9A3rpDKkGkLEaNQimEhQkyNOV--gQact6QlzcxMvs_p6VN_IHUXVwKffpgNf_Tta9NAHSi0hMhpnXA2pOVSewVHPaCGP3Kbt7Sy5yZLbH_cZ_h_mecJsM-ZRbn91nj-yH8eBdrKgzvpLbr_6foQ7bE8aZ8huhDzIAo29wMb6Xg-Yx49s6joZPcEeRthe7icXZu8C_ezJqbSR0xAwWZpaHD9_sKOQdzkg79LTSJoTgYFO2dgZXIzgg1yzHwIxKGvIRVDeOVJzPpnh1UuOK4lNa-SKSZH5m-eJUzuFIc7Ct_J9qcW8VMpuZjOhDu8RTZ8nbTBNMynsU31bOnEtWuIWjU5f8zU_lSI21FIplqnUyC-lKN1bwmEf_fHilFhG7ula9J1OnhUGa8ZWLCu0gd7-ANel7tU?type=png&quot; alt=&quot;drawing&quot; style=&quot;max-width:500px;&quot; /&gt;
&lt;figcaption&gt;Flow diagram for the MIDI Archive&lt;/figcaption&gt;
&lt;!-- source: https://mermaid.live/edit#pako:eNplUsFqGzEQ_ZVBJwXiQujNh4LttR1DCgUHmqSbw1ia3RXRStuR1ls35N8r7W5JQ3UaoffezLynV6G8JrEUlfWDapAj3Belg3RW8qgYO-IrWCy-PMnCD8561PD1UBygMpYCVOxbOLhI7CjCilVjznQ18Z8yDdbySHymiYQhUAxwNgh7E5v-9A3rpDKkGkLEaNQimEhQkyNOV--gQact6QlzcxMvs_p6VN_IHUXVwKffpgNf_Tta9NAHSi0hMhpnXA2pOVSewVHPaCGP3Kbt7Sy5yZLbH_cZ_h_mecJsM-ZRbn91nj-yH8eBdrKgzvpLbr_6foQ7bE8aZ8huhDzIAo29wMb6Xg-Yx49s6joZPcEeRthe7icXZu8C_ezJqbSR0xAwWZpaHD9_sKOQdzkg79LTSJoTgYFO2dgZXIzgg1yzHwIxKGvIRVDeOVJzPpnh1UuOK4lNa-SKSZH5m-eJUzuFIc7Ct_J9qcW8VMpuZjOhDu8RTZ8nbTBNMynsU31bOnEtWuIWjU5f8zU_lSI21FIplqnUyC-lKN1bwmEf_fHilFhG7ula9J1OnhUGa8ZWLCu0gd7-ANel7tU --&gt;
&lt;/figure&gt;
&lt;p&gt;Since the details of this loose architecture diagram are subject to change, I’ll focus on it more in my next post, and instead would like to reflect more on the &lt;code&gt;AWS Lambda&lt;/code&gt; portion of the diagram.&lt;/p&gt;
&lt;p&gt;I’m not yet sure if this project is something I’ll keep working on, or if I’ll let it stay in some form as a proof of concept, but I’d like to deploy in a low-cost and low-maintenance way that would allow me to let it passively exist online with minimal billing and upkeep. While there’s lots of options to deploy ML models to production, I set my sights on Lambda early on because of its usage-based pricing model where I can have the service turned off from most of the day. The model I’ve built so far has pretty limited utility, certainly not sophisticated enough to be useful for generating actual music or aiding the process of composing, so there’s little to warrant it having very much server uptime.&lt;/p&gt;
&lt;p&gt;But this limitation also feels ripe for some creative elaboration, and I’ve been circling around the idea of having the ML model present daily “performances”, in which visitors to the site can hear some music generated by the model. The general inspiration of this comes from &lt;a href=&quot;https://solar.lowtechmagazine.com/&quot;&gt;Low Tech Magazine&lt;/a&gt;, which is hosted on a solar-powered server that sometimes goes down if the weather is cloudy for too long and the battery depletes. Or &lt;a href=&quot;https://radioamnion.net/&quot;&gt;Radio Amnion&lt;/a&gt;, which only broadcasts for a few days around each new moon. There’s something beautiful about these websites that have variable behavior depending on time or atmospheric conditions.&lt;/p&gt;
&lt;p&gt;As for right now, the plan is to have two separate Lambda functions, each of which is only invoked once a day. The first is responsible for serving the machine learning model, and will generate a MIDI sequence that is stored in S3. The second is then responsible for broadcasting that MIDI file over websockets to visitors to the site. Lambdas have a maximum execution time of 15 minutes, which will also limit the duration of the broadcast. An important detail of this, too, is that all visitors to the site will experience the same broadcast at the same time, which to me makes it more of a “performance” than just letting visitors play the MIDI file as they please.&lt;/p&gt;
&lt;p&gt;In terms of implementation, I’ve been struggling to get the model hosted in Lambda, as I hadn’t quite realized how &lt;strong&gt;beefy&lt;/strong&gt; the &lt;code&gt;PyTorch&lt;/code&gt; library is, which I’ve been using to build and train the model, and is far too large for Lambda’s filesize limitations. At first I tried to use an existing tool called &lt;a href=&quot;https://github.com/szymonmaszke/torchlambda&quot;&gt;torchlambda&lt;/a&gt; to minimize and deploy my trained model, but should have taken the fact that it hadn’t been updated in three years as a red flag. In that time, PyTorch has gone through a major version change, and made torchlambda rather unsuitable.&lt;/p&gt;
&lt;p&gt;At the same time, it was interesting to me that torchlambda used something called &lt;a href=&quot;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&quot;&gt;torchscript&lt;/a&gt; to compile a model to C++, and vastly minimize the size of source code and dependencies to be deployed to Lambda. But as I was considering going further down this path, &lt;a href=&quot;https://github.com/kenjinp&quot;&gt;Kenny&lt;/a&gt;, a fellow Recurse resident suggested that I look into &lt;a href=&quot;https://onnx.ai/&quot;&gt;https://onnx.ai/&lt;/a&gt;, which aims to bring interoperability to the growing ecosystem of various ML libraries and runtime environments. The Onnx runtime, unlikely PyTorch, is relatively small (~45 MB), and falls within Lambda’s filesize restrictions. After a quick test run, loosely following suggestions &lt;a href=&quot;https://becominghuman.ai/onnx-inference-with-python-in-aws-lambda-f09d08530e87&quot;&gt;here&lt;/a&gt;, I’ve set up a toy model deployed to Lambda using the Onnx runtime packaged into a Lambda layer, and happy to report that it’s functioning as expected!&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Goldberg Variations Variations</title>
    <link href="https://reubenson.com/recurse/week-4/"/>
    <updated>2025-05-28T07:46:24Z</updated>
    <id>https://reubenson.com/recurse/week-4/</id>
    <content type="html">&lt;h3&gt;Week 4 at Recurse Center&lt;/h3&gt;
&lt;p&gt;After seeing my friend Asha Tamirisa give a talk on building a &lt;a href=&quot;https://thekitchen.org/on-view/counter-archiving-the-avant-garde/&quot;&gt;counter-archive of The Kitchen&lt;/a&gt; at &lt;a href=&quot;https://pioneerworks.org/programs/software-for-artists-day-8&quot;&gt;Software for Artists Day&lt;/a&gt; last week, I’ve been thinking more about the overlap between archives and ML training sets, how each deals with concerns of how information perpetuates biases, and produces contestable views of history and reality. Complex socio-economic and political realities are embedded in the objective artifacts that may be included or excluded in both archives and training sets.&lt;/p&gt;
&lt;p&gt;The emergence of certain canonical training sets, like &lt;a href=&quot;https://www.kaggle.com/datasets/wcukierski/enron-email-dataset&quot;&gt;this archive of half a million emails within Enron&lt;/a&gt; obtained by the federal government during its investigation, says something too about how ML is a powerful tool for distilling and operationalizing archives. To turn archives of the near or distant past into substrates that can be mined for the purposes of future prediction or content generation.&lt;/p&gt;
&lt;p&gt;In the information age, big tech often finds a business model in leveraging capital to extract value from assets already in circulation in the wider economy (e.g.ride-sharing or behavioral surplus), and this continues to be the pattern in how today’s LLMs feed on the previously inert archives of the past.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I listened to the music. It was hideous. I have never heard anything like it. It was distorted, diabolical, without sense or meaning, except, perhaps, an alien, disconcerting meaning that should never have been there. I could believe only with the greatest effort that it had once been a Bach Fugue, part of a most orderly and respected work.&lt;/p&gt;
&lt;p&gt;(Excerpt from Philip K. Dick’s &lt;em&gt;The Preserving Machine&lt;/em&gt;, thanks &lt;a href=&quot;https://laurelschwulst.com/&quot;&gt;Laurel&lt;/a&gt;!)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This last week, I’ve been buiding a neural net model trained on MIDI transcriptions of Bach to produce Bach-like music, though not very well at the time of this writing. Around this time last year, I was digging up old MIDI websites from the 1990s, of which I can imagine only a fraction are still online. There are some sizeable training sets available for purposes like this, but I’ve been returning to my old bookmarks instead, and so far have been relying on MIDI transcriptions shared on &lt;a href=&quot;http://www.jsbach.net/&quot;&gt;Dave’s J.S. Bach Page&lt;/a&gt; (first launched in 1996, and last updated in 2010). While revisiting this beautiful website, I came across a figure named John Sankey, known then as the &lt;a href=&quot;https://johnsankey.ca/harpsichord.html&quot;&gt;&lt;em&gt;Harpsichordist to the Internet&lt;/em&gt;&lt;/a&gt;. I was also surprised to see brief mention in his writings on his personal experience in having his &lt;a href=&quot;https://johnsankey.ca/bach.html&quot;&gt;MIDI files stolen and commercialized&lt;/a&gt;, before the longer arc of music piracy during the MP3 age and the rise of Spotify. I’d largely thought of this early period of music file-sharing on the web as a more wholesome era, but it’s helpful to remember that the incentives have been such that much smaller actors than the tech behemoths of today have long taken advantage of opportunities to build products around freely available information on the web.&lt;/p&gt;
&lt;p&gt;With all this in mind, and with my remaining two weeks at Recurse, I’d like to spend a little more time exploring the question of how ML may work towards or alongside archives, rather than ingest them entirely, tracing the faultlines between the archive and the training set.&lt;/p&gt;
&lt;!-- 

been thinking more about how the problematics of archives (tk) bleeds over into the questions of perpetuated biases within training sets for ML systems

- Are ML systems just archives at scale?
- So much effort is leveraged into producing a corpus of training data, but has often been built out of what&#39;s simply available (enron emails?)
- Big tech has always been prone to leveraging capital in order to extract capital from assets already in circulation in the wider economy (the ride-sharing model, the labor behind maintaining Wikipedia)
- A smaller example of the above is found in Sankey, who I discovered while exploring the early music web. How, even then, smaller actors than the tech behemoths today sought out arbitrage opportunities to build products around freely available information
- In the remaining two weeks at Recurse, I&#39;d like to spend a little more time exploring the question of how machine learning may work alongside the project of archive-building, of what divergences may exist between the archive and the training corpus. --&gt;</content>
  </entry>
  
  <entry>
    <title>Alignment Is All You Need</title>
    <link href="https://reubenson.com/recurse/week-3/"/>
    <updated>2025-05-28T07:46:24Z</updated>
    <id>https://reubenson.com/recurse/week-3/</id>
    <content type="html">&lt;h3&gt;Week 3 at Recurse Center&lt;/h3&gt;
&lt;p&gt;I’ve just finished my third week at Recurse Center, which means I’m already halfway through my residency here. Although I’m still digesting the material in &lt;a href=&quot;https://karpathy.ai/zero-to-hero.html&quot;&gt;Andrej Karpathy’s Neural Net series&lt;/a&gt;, I’ve finished working through the videos, and in the coming week, I’ll be continuing to implement my own model system to improve my understanding and intuition about these systems.&lt;/p&gt;
&lt;p&gt;The last video in the series is centered around the pivotal &lt;a href=&quot;https://reubenson.com/recurse/week-3/&quot;&gt;Attention is All You Need&lt;/a&gt; paper, which presents the &lt;em&gt;transformer&lt;/em&gt; architecture central to OpenAI’s GPT models. In working through Karpathy’s lecture series, I’m struck by the elegance of neural net architecture, their composition through the repetition of simple elements. And I’m struck too by the degree to which much of the “art” of designing these systems relies on simple arithmetic operations that nudge the state of the system within a regime of being “well-behaved”. Scale is the main thing, which led Rich Sutton to conclude in his &lt;a href=&quot;http://www.incompleteideas.net/IncIdeas/BitterLesson.html&quot;&gt;&lt;em&gt;Bitter Lesson&lt;/em&gt;&lt;/a&gt;: “We have to learn the bitter lesson that building in how we think we think does not work in the long run.”&lt;/p&gt;
&lt;p&gt;Meanwhile … OpenAI continues to make dramatic headlines, with the sudden firing of Sam Altman. At the time of this writing, conflicts within the board around the speed and safety of the development of their products seem to be at the core of this rift.&lt;/p&gt;
&lt;p&gt;The question of social disalignment around the topic of &lt;em&gt;AI Alignment&lt;/em&gt; within the leadership team at OpenAI is poignant, provocative, and perhaps all too predictable … disalignment around what it means to build in how we think. Institutions have emergent behaviors and a kind of artificial intelligence too. It seems worth pondering how theories around the problem of alignment in both technologic and social domains emerged at a similar time, as is explored by Orit Halpern in a &lt;a href=&quot;https://www.journals.uchicago.edu/doi/10.1086/717313&quot;&gt;recent paper on the conjoined histories of neural nets and Hayekian neoliberal economics&lt;/a&gt;. Which is to say, &lt;a href=&quot;https://www.reddit.com/r/singularity/comments/17yxl1x/the_head_of_applied_research_at_openai_seems_to/&quot;&gt;e/acc&lt;/a&gt; is nothing new, and alignment has perhaps been out of vogue for nearly a century.&lt;/p&gt;
&lt;p&gt;In my own microcosm of space-time at Recurse, the question of alignment (on a personal scale) feels like a desire for the reassertion of control: in optimizing my time here, having finished projects, the setting and completion of goals. Alignment presumes there’s a reward or loss function at the heart of the model/subject (as is the case for both neural nets and neoliberal economics), but the archetype of the &lt;a href=&quot;https://miguelabreugallery.com/wp-content/uploads/2016/12/GroundingVision_MAG_2017_Sequence_06-1-1024x276.jpg&quot;&gt;meandering line&lt;/a&gt; still feels more appropriate to me. And I wonder, too, about the inverse of Sutton’s &lt;em&gt;Bitter Lesson&lt;/em&gt;, of how the things we build changes how we think, how the logic embedded in formal systems become heuristics for our own sense-making.&lt;/p&gt;
</content>
  </entry>
  
  <entry>
    <title>Gradually, Then Suddenly</title>
    <link href="https://reubenson.com/recurse/week-2/"/>
    <updated>2025-05-28T07:46:24Z</updated>
    <id>https://reubenson.com/recurse/week-2/</id>
    <content type="html">&lt;h3&gt;Week 2 at Recurse Center&lt;/h3&gt;
&lt;p&gt;Earlier this week, at an informal dinner centered around the theme of AI Safety, I saw Rob Nail of Singularity University quote &lt;em&gt;“gradually, then suddenly”&lt;/em&gt; (attribution often asigned to Ernest Hemingway, on going bankrupt) in a presentation. It feels equally apt describing the subjective experience of time while at Recurse Center as it does exponential technologies. While I’d been vaguely familiar with AI Safety, this week has gone a long ways towards concretizing my understanding of this emerging field of research and development (thanks in no small part to &lt;a href=&quot;https://github.com/changlinli&quot;&gt;Changlin Li&lt;/a&gt;’s generous efforts to bring me and my cohorts at Recurse up to speed!).&lt;/p&gt;
&lt;p&gt;As of right now, I’ve made it through 4 of the 7 &lt;a href=&quot;https://karpathy.ai/zero-to-hero.html&quot;&gt;videos in Andrej Karpathy’s lecture series on neural nets&lt;/a&gt;, and the first two chapters of the &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;fastbook notebooks&lt;/a&gt;. The majority of my effort is still towards getting up to speed on fundamentals. But that said, it seems worthwhile to think through some of my current questions, assumptions, and intuitions here, if only for my future benefit in tracking how my views shift over time.&lt;/p&gt;
&lt;h3&gt;On AGI&lt;/h3&gt;
&lt;p&gt;AGI (Artificial General Intelligence) seems like a rather squishy term that is dependant on significant, fundamental vagaries: around the nature of our own intelligence, and the question of interpreting/measuring intelligence of an AI system. But it does seem plausible (and probable even) that if we &lt;em&gt;do&lt;/em&gt; build an AGI, we’ll do so before fully understanding how that emergent behavior was achieved. And it will be even longer still, before any meaningful consensus is developed around whether AGI has actually been achieved.&lt;/p&gt;
&lt;p&gt;So given the long arc of developing and recognizing systems that approach AGI, I wonder what it would mean to give space to such systems?&lt;/p&gt;
&lt;!-- In the longer arc of developing systems toward AGI , I wonder what it would mean to give space to such systems --&gt;
&lt;!-- As funding and development of AI outpace AI Safety,  --&gt;
&lt;!-- 
So ... will we reach AGI or what

I have inclinations to debate some of the terminology involved here; ground truth on words that are routinely used to describe current and future technology is fundamentally impossible to pin down. Shortlist of such words: learning, intelligence, artificial, human | machine. And to answer this question is truly beyond me, and I&#39;m happy enough in this case to defer to experts. In some ways, I&#39;m not really interested to address this question at all. But I do wonder what it means to give space to such systems? --&gt;
&lt;p&gt;From what I’ve learned of ML systems thus far, they’re tightly bounded by two constraints: data and computing resources. Model architectures optimize across these two constraints, and the success of current state of the art (LLMs like GPT) relies on transformer-based architecture which rapidly advanced such optimizations. While current transformer-based models may be transient (and there are reasons to hope that they’ll be replaced by architectures that are easier to evaluate according to AI Safety standards), all future architectures will continue to be constrained by data and computing resources on some fundamental level. As these systems scale up in capability, the bottlenecks between subsequent developments will come from doing the most with what data these systems have access to, maximizing the latent knowledge built up from data inflows.&lt;/p&gt;
&lt;!-- As of right now, images and text are the richest sources of data that we have, due to their representational efficiency. --&gt;
&lt;p&gt;Given this, what happens in the limit as we approach AGI? We may currently be in the regime of low-hanging fruit as current state of the art is trained on the internet as its corpus, but richer data-mining would come from greater interaction between models and humans. Heterogeneous systems comprised of humans and AIs interacting would therefore be incentivized from both a training and utility perspective.&lt;/p&gt;
&lt;p&gt;Such heterogeneous systems might perhaps start out being more human-centric. For example, let’s say Apple deploys a new OS that has an AI agent that can automatically answer your emails upon prompting you with a simple question here and there. Much like no one wants an email signature that reads &lt;code&gt;Sent from Reuben&#39;s iPad&lt;/code&gt;, users typically won’t want to send emails that obviously read as being AI-generated. Therefore, in this heterogeneous system, the user will spend time training the AI properly, in order to maintain consistency of tone. Along this path, there will be a tipping point in which it is much easier to give the AI full control over writing one’s emails, than to shut it off and go back to writing your own.&lt;/p&gt;
&lt;p&gt;This in itself doesn’t seem like such a bad thing, but as heterogeneous systems increase in scale, breadth, and interconnectivity, a tendency will exist to make interfaces more and more uniform, to be accessible to both human and AI agents. For example, a new housing development may design an apartment layout that optimizes for human preference and AI inferrability, or a municipalities may decide to revamp its roadways in a way that accomodates driverless cars alongside human drivers.&lt;/p&gt;
&lt;p&gt;The more we increase the surface area of interconnected complex heterogeneous systems (AI augmenting each layer of &lt;a href=&quot;https://thestack.org/&quot;&gt;&lt;em&gt;The Stack&lt;/em&gt; as defined by Benjamin Bratton&lt;/a&gt;), the more difficult it may be to unplug a given set of AI technologies without risking cascading failures across many systems. &lt;a href=&quot;https://ourworldindata.org/ai-timelines&quot;&gt;Many AI researchers seem to be reaching consensus about human-level AI being achieved within 100 years&lt;/a&gt;, but trends towards uniform interfaces will happen long before AGI’s arrival.&lt;/p&gt;
&lt;!-- Such heterogeneous systems might then be characterized in the following ways: --&gt;
&lt;!-- - More uniform interfaces, to be accessible to both humans and AI agents. For example, a new housing development may design an apartment layout that optimizes for human preference and AI inferrability --&gt;
&lt;!-- As a result, I would speculate some of the following to be the case, in the optimization of AGI (along the constraints of compute and data) within the context of increasingly large surface areas of heterogeneous systems inb which humans and actively-training models coexist --&gt;
&lt;!-- - More uniform interfaces (e.g. changing roadway infrastructure to balance the requirements of machine vision and human vision) --&gt;
&lt;!-- - Terminology seems like a fundamentally contentious complex issue when talking about AI. I&#39;m still hesitant to even describe the current technology as AI, as opposed to Machine Learning. At the asymptote of the current development, I&#39;d be more inclined to describe the resulting technology as Machine Intelligence, as opposed to Artificial Intelligence.  --&gt;
&lt;!-- -  --&gt;</content>
  </entry>
  
  <entry>
    <title>Week 1 at Recurse Center (Learning to Paddleboard)</title>
    <link href="https://reubenson.com/recurse/week-1/"/>
    <updated>2025-05-28T07:46:24Z</updated>
    <id>https://reubenson.com/recurse/week-1/</id>
    <content type="html">&lt;p&gt;Twice this year, I’ve had the good fortune to find myself in spaces filled with people aligned to a common purpose: earlier this year at &lt;a href=&quot;https://www.haystack-mtn.org/&quot;&gt;Haystack Mountain Schoool of Craft&lt;/a&gt;, and now at &lt;a href=&quot;https://www.recurse.com/&quot;&gt;Recurse Center&lt;/a&gt;. These environments foster a regime of expansion and change, and have the power to focus, broaden, and alter one’s sense of purpose. These environments can be quite intense to adjust and recalibrate to, as they involve large changes to the core substrates and infrastructures that compose of one’s daily existence: time, geography, architecture, and community. Much of this is self-evident and not particularly profound to point out, but its lived experience can most immediately be felt in the constellation of affective states that manifest throughout the transition: overhwelm, nervous excitement, disorientation, decision paralysis, fight-or-flight, hyperactivity, dissociation.&lt;/p&gt;
&lt;p&gt;This summer, at Haystack, in addition to learning the &lt;a href=&quot;https://medium.com/@reubenson/foray-into-3d-printing-with-clay-at-haystack-207064511cd&quot;&gt;workflows and processes of ceramic 3D printing&lt;/a&gt;, I went paddleboarding for the very first time. On my initial attempts to stand up and paddle about, I was quickly tossed into the water. The Haystack campus is built on a granite island off the Maine coast, and faces out directly to the ocean. After falling a couple times into the rocking waves, I decided to paddle sitting down instead for a while, but before finally returning to shore, I decided to just stand on the board for a bit, without paddling. I then sat back down again and rowed back to shore. The next day, and each subsequent day on the beach, I found myself immediately able to paddleboard standing up without any further issue. The last time I watched my body in the process of learning was when I began taking ceramics classes in 2019, and it struck me, more or less for the first time, how quickly, and quietly, embodied intelligence is set into motion.&lt;/p&gt;
&lt;p&gt;I think of acclimating to environments like Recurse in a similar way, that the transition still passes through the body, even for something as heady as Recurse’s guiding mission to enable its participants to become radically better programmers over the duration of the residency.&lt;/p&gt;
&lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt;
&lt;p&gt;So, it’s been a lot of just that, riding the waves through varying affective states, while also slowly refining my sense of purpose here. I’ve been working out a set of priorities and goals, in order to make more tangible that sense of purpose, which I’ll share below. It’s a little sobering that after having just wrapped the first week of my six-week residency here, I’m already nearly a quarter of the way through.&lt;/p&gt;
&lt;p&gt;So far, I’ve committed to going through some amount of &lt;a href=&quot;https://karpathy.ai/zero-to-hero.html&quot;&gt;Andrej Karpathy’s introduction to building neural networks from scratch&lt;/a&gt; and Russell Webb’s &lt;a href=&quot;https://github.com/Russ741/karpathy-nn-z2h&quot;&gt;accompanying worksheets&lt;/a&gt;, though I’m also likely going to switch to or ping-pong between that series and the &lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;Fast.ai series&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Guiding principles&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Stay open-ended and cultivate sensitivity, paying attention to what drives my own interests in tech, and see what it is that drives other programmers to similar or divergent interests&lt;/li&gt;
&lt;li&gt;Think of the residency as more of an onboarding, not an end to itself&lt;/li&gt;
&lt;li&gt;Embrace being process-oriented, balanced against being results-driven (but not necessarily working backwards from a predetermined outcome)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Goals&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn a new programming language (Python)&lt;/li&gt;
&lt;li&gt;Learn the fundamentals of ML&lt;/li&gt;
&lt;li&gt;Get feedback on previous/existing projects, like &lt;a href=&quot;https://frogchor.us/&quot;&gt;Frog Chorus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Finish and share at least one small project&lt;/li&gt;
&lt;li&gt;Understand current employment landscape / opportunities&lt;/li&gt;
&lt;li&gt;Give focus to both writing code and text (like this entry)&lt;/li&gt;
&lt;/ul&gt;
</content>
  </entry>
  
  <entry>
    <title>Reflections on my time at Recurse Center</title>
    <link href="https://reubenson.com/recurse/reflections/"/>
    <updated>2025-05-28T07:46:24Z</updated>
    <id>https://reubenson.com/recurse/reflections/</id>
    <content type="html">&lt;p&gt;(Note: this writing has also been published to Medium &lt;a href=&quot;https://medium.com/@reubenson/archives-ai-and-music-of-the-early-web-9b2f51fdef47&quot;&gt;here&lt;/a&gt;)&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;https://images.metmuseum.org/CRDImages/ep/original/DT2159.jpg&quot; alt=&quot;painting of flowers by painter Odilon Redon&quot; style=&quot;max-width:420px; margin: auto;&quot; /&gt;
    &lt;figcaption&gt;&lt;a href=&quot;https://www.metmuseum.org/art/collection/search/437382&quot; target=&quot;_blank&quot;&gt;Odilon Redon, Vase of Flowers (Pink Background), ca. 1906 &lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This past autumn, I spent a six-week residency at &lt;a href=&quot;https://recurse.com/&quot;&gt;Recurse Center&lt;/a&gt;, a retreat where “curious programmers recharge and grow.“ Earlier that year, I had left my post at &lt;em&gt;Vox Media&lt;/em&gt; as an engineering manager, and in the intervening months, had been exploring the world of ideas and software, as if anew. Accordingly, I arrived at Recurse with a long list of things I might like to learn, and in a swirl of curiosity and nervousness, I proceeded to give myself the space to take on Recurse Center’s core directive of &lt;em&gt;becoming a dramatically better programmer&lt;/em&gt;.&lt;/p&gt;
&lt;!-- , knowing that balancing focus with breadth would be a challange. I had spent --&gt;
&lt;!-- curious programmers recharge and grow, this fall and feel a pang of anxiety about what I didn&#39;t manage to do. I arrived here after having not worked as an IC in over two years, and accordingly, I come with a long list of things I might like to learn, and balancing focus with breadth was to be a real challenge. --&gt;
&lt;!--  --&gt;
&lt;!-- Questions, both big and small, loomed over me, and in a swirl of curiosity and nervousness, I proceeded to give myself the space to take on Recurse Center&#39;s core directive of ```becoming a dramatically better programmer```, however I might interpret that. --&gt;
&lt;!-- &lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt; --&gt;
&lt;h3&gt;Archives and AI&lt;/h3&gt;
&lt;p&gt;I found at Recurse a community of highly motivated and brilliant technologists, all of us excited to pursue projects beyond the constraints of professional employment, and to learn something about ourselves in the process. Like many others, AI was on my bingo board of potential areas of exploration, and I decided to join up with a study group around the fundamentals of machine learning. We took &lt;a href=&quot;https://karpathy.ai/zero-to-hero.html&quot;&gt;Andrej Karpathy’s video series on neural nets&lt;/a&gt; as our curriculum, working our way up from the simplest neural net to a transformer model, based on the landmark &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention is All You Need&lt;/a&gt; paper. The emergent behavior from simple mechanics of &lt;a href=&quot;https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b&quot;&gt;back-propagation&lt;/a&gt; and &lt;a href=&quot;https://karpathy.github.io/neuralnets/#:~:text=commonly%20referred%20as-,Stochastic%20Gradient%20Descent,-.%20The%20interesting%20part&quot;&gt;stochastic gradient descent&lt;/a&gt; fascinated me, as did the expressiveness of entropy introduced through techniques like &lt;a href=&quot;https://en.wikipedia.org/wiki/Batch_normalization&quot;&gt;batch normalization&lt;/a&gt;. I was reminded of my earlier years, studying physics and working in a research lab, when I first encountered model systems as an engine for advancing scientific understanding. A successful model system balances generality and specificity, such that it expresses an underlying broader truth without including all the complexity of the &lt;em&gt;real world&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;But also, as the neuropsychiatrist and philosopher Iain McGilchrist writes, “&lt;a href=&quot;https://www.goodreads.com/author/quotes/1045743.Iain_McGilchrist?page=1#:~:text=%E2%80%9CThe%20model%20we%20choose%20to%20use%20to%20understand%20something%20determines%20what%20we%20find.%E2%80%9D&quot;&gt;The model we choose to use to understand something determines what we find&lt;/a&gt;.” The questions of “what we find“ is partially addressed by research around &lt;a href=&quot;https://www.anthropic.com/index/core-views-on-ai-safety&quot;&gt;AI Safety and Alignment&lt;/a&gt;, but the discourse around what we are &lt;em&gt;looking for&lt;/em&gt; in AI models is much more diffuse. I sometimes think about how much of Ray Kurzweil’s pioneering research into &lt;a href=&quot;https://en.wikipedia.org/wiki/Kurzweil_K250&quot;&gt;music&lt;/a&gt; and AI seems to &lt;a href=&quot;https://www.youtube.com/watch?v=ZlhYY3z5Hv8&quot;&gt;point back to the death of his father&lt;/a&gt;, and how themes of mortality and history commonly reappear in topics like existential risk from AI and the cultivation of data to be used in model training. With these questions in mind, I found my attention wandering from AI towards archives, which now live a double life as training sets for ML models.&lt;/p&gt;
&lt;!-- But I also found that developing a first-principles understanding of the material only got me so far, and it was challenging to wrap my head around the broader existential questions around this technology.  --&gt;
&lt;!-- These questions are partially addressed within the frameworks of AI Safety and Alignment, but I also found my attention wandering to the question of archives, which now live a double life as training sets for ML models. --&gt;
&lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt;
&lt;p&gt;In our age of planetary-scale existential precarity, archives seem to have come into a kind of fashion, offering a degree of solace in feeling knowable, static, and grounding. The training procedures of AI foundation models like GPT have a close relationship with archives too, but instead rely on their accessibility, volume, and givenness, which allows archives to be composed and instrumentalized as training sets. Abstractions of archives and AI are not impermeable, however, and as I began building my transformer model, I decided to take a broad approach to my introduction to machine learning. I began to construct an archive alongside the model, in order to see how the project might evolve through the accumulation of design decisions regarding the archive and the mechanics of the machine learning model.&lt;/p&gt;
 &lt;!-- concrete mechanadd leakiness to abstraction, and to explore the concrete mechanisms and  the distinction between them, between conservation and generation. --&gt;
&lt;figure&gt;
&lt;img src=&quot;https://reubenson-portfolio.s3.us-east-1.amazonaws.com/assets/Pipkin-curation.jpg&quot; alt=&quot;screenshot of quote from Everest Pipkin on curating a training set&quot; style=&quot;max-width:420px; margin: auto;&quot; /&gt;
&lt;figcaption&gt;Screenshot from Everest Pipkin&#39;s &lt;a href=&quot;https://www.youtube.com/watch?v=IYNKs8vfocc&quot; target=&quot;_blank&quot;&gt;Corpora as medium: on the work of curating a poetic textual dataset&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In Everest Pipkin’s &lt;a href=&quot;https://www.youtube.com/watch?v=IYNKs8vfocc&quot;&gt;Corpora as medium&lt;/a&gt; talk, they describe the construction of a training set for machine learning as an act of curation, and remind us that &lt;em&gt;curation&lt;/em&gt; comes from the Latin word for care, &lt;em&gt;cura&lt;/em&gt;. More than conveying the old adage &lt;em&gt;garbage in garbage out&lt;/em&gt;, I take this as an insight about the &lt;em&gt;aura of provenance&lt;/em&gt; in the age of synthetic media. Critiques of generative AI as “&lt;a href=&quot;https://twitter.com/GalaxyKate/status/1583907942834716672&quot;&gt;Bach Faucets&lt;/a&gt;” seem to give undue priority to the &lt;em&gt;sui generis&lt;/em&gt; quality of a work, and not enough weight to its ability to speak to  (for) a corpus. Instead, my sense is that AI will increasingly take on the quality of an archive (either institutional or personal), as its oracle, in which the artifacts it generates will articulate something that is confusingly at once generative &lt;em&gt;and&lt;/em&gt; derivative.&lt;/p&gt;
&lt;!-- the generative creativity of humans wielding AI tools  --&gt;
&lt;!-- the artifacts of generative AI won&#39;t be  --&gt;
&lt;!-- I take this to be a deeper reading of the phrase &quot;attention is all you need&quot;, suggestive of  --&gt;
&lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt;
&lt;p&gt;Prior to my professional career as a software engineer, I wrote music software for myself, &lt;a href=&quot;https://reubenson.com/qcvg/&quot;&gt;programming in C++ for Arduino to control electronic music instruments&lt;/a&gt;. Combining software development with sound art has been a continuous thread for me throughout the intervening decade, with projects like &lt;a href=&quot;https://www.youtube.com/watch?v=ERbfczLUr-A&quot;&gt;HOBO UFO&lt;/a&gt;, &lt;a href=&quot;https://reubenson.com/weaving/&quot;&gt;Weaving Music&lt;/a&gt;, and most recently, &lt;a href=&quot;https://frogchor.us/&quot;&gt;Frog Chorus&lt;/a&gt;. Computer music pioneer John Bischoff once wrote about &lt;em&gt;&lt;a href=&quot;https://direct.mit.edu/lmj/article-abstract/doi/10.1162/lmj.1991.1.37/63238/Software-as-Sculpture-Creating-Music-from-the&quot;&gt;Software as Sculpture&lt;/a&gt;&lt;/em&gt;, and this feels rather resonant to me, that writing software is not as linear as it sometimes sounds, and that undertaking creative software projects is  about emphasizing the qualities of exploration and expression that is present in all software development. In this tradition, I found myself looking to music on the early web as a focal point for developing the archive that would also serve as the training set for my ML model project, as an act of both &lt;em&gt;curation&lt;/em&gt; and &lt;em&gt;care&lt;/em&gt;.&lt;/p&gt;
&lt;!-- Before the age of MP3s, music on the web was dominated by MIDI, and I felt an impulse to give this history a closer look, in order to understand something about the drive of technological transformation in the current moment.   --&gt;
&lt;h3&gt;Searching the future for what exists in the past&lt;/h3&gt;
&lt;p&gt;Before MP3s came to dominate how people listen to music on the internet, the sounds of the early web (&lt;a href=&quot;https://forums.theregister.com/forum/all/2019/07/12/a_pair_of_usenet_pirates_get_66_months_behind_bars/#:~:text=Usenet%2C%20that%20brings%20back%20memories.%20Used%20to%20use%20it%20when%20I%20started%20at%20Uni.%20Great%20source%20of%20mod%20and%20midi%20files%2Cnone%20of%20that%20new%20fangled%20MP3%20nonsence!&quot;&gt;and even BBS and Usenet before the world wide web&lt;/a&gt;) were predominantly expressed via MIDI. Its tiny file-size was accomodated by bandwidth limitations of the 1980s and 90s, and web-native support for the MIDI file format came early from browsers like &lt;a href=&quot;https://www.vice.com/en/article/a359xe/the-internets-first-hit-file-format-wasnt-the-mp3-it-was-midi#:~:text=In%20particular%2C%20Microsoft%E2%80%99s%20Internet%20Explorer%20supported%20it%20as%20far%20back%20as%20version%201.0%2C%20while%20Netscape%20Navigator%20supported%20it%20with%20the%20use%20of%20a%20plug%2Din%20and%20added%20native%20support%20starting%20in%20version%203.0.&quot;&gt;Internet Explorer and Netscape Navigator&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The MIDI files collected for this project and used to train the model were once very new. In sifting through them, I’ve been searching for the feeling of technological transformation in an earlier age, how they combine the possibilties of new aesthetic experiences with the technics of producing and distributing this particular format of media. In giving a closer look to this history, I’ve hoped to uncover something playful and persistent about humanity’s drive towards technological transformation.&lt;/p&gt;
&lt;p&gt;Of the artists who shared their MIDI work in the early years of the web, the researcher and musician &lt;a href=&quot;https://johnsankey.ca/&quot;&gt;John Sankey&lt;/a&gt;, who was once known to many as &lt;a href=&quot;http://www.jsbach.net/midi/midi_johnsankey.html&quot;&gt;The Harpsichordist to the Internet&lt;/a&gt;, conveys a brief, but poignant retelling of how his MIDI recordings were appropriated by a commercial entity. His experience swayed him away from sharing work directly on the open web going forwards, a reactionary turn that perhaps presages what &lt;a href=&quot;https://studio.ribbonfarm.com/p/the-extended-internet-universe&quot;&gt;Venkatesh Rao has described as the cozy web&lt;/a&gt; and &lt;a href=&quot;https://onezero.medium.com/the-dark-forest-theory-of-the-internet-7dc3e68a7cb1&quot;&gt;Yancey Strickler calls &lt;em&gt;dark forest spaces&lt;/em&gt;&lt;/a&gt; after the emergence of monolithic Web2 platforms.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“I’m far from the first musician whose heart has been touched by Bach’s music, and I won’t be the last. I’ve played all of his harpsichord music at one time or another, and started to record it. Then a creep rubber-banded the tempi, pretended they were his and were played on a piano, legally copyrighted the results in the USA, and threatened legal action against sites that refused to carry them […] The sites that carry his files know what happened, and don’t care … I play for myself and friends now.” - &lt;a href=&quot;https://johnsankey.ca/bach.html&quot;&gt;John Sankey&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It’s not without intended irony that I’ve included John Sankey’s MIDI performances of Bach in the training set for this model (though, &lt;a href=&quot;https://arxiv.org/pdf/1812.06669.pdf&quot;&gt;I’m certainly not the first either&lt;/a&gt;). MIDI files from this time are highly technical objects requiring specialized hardware and software in addition to musicanship, but yet now overloaded with valences of nostalgia and kitsch. The juxtaposition of these qualties with machine learning felt necessary to me, in balancing sensibilities, and captures something about how linear technologic time presents history as a series of un-fulfilled futures.&lt;/p&gt;
&lt;!-- &lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt; --&gt;
&lt;!-- But also to explore questions of how AI and archives will continue to complement each other, and how the era of generative art comes with expansive and difficult questions of _where_ art is generated _from_ ... and what is this all _for_? --&gt;
&lt;!-- With this context in mind, I began developing a machine learning model trained on music from the early web.  --&gt;
&lt;!-- &lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt; --&gt;
&lt;h3&gt;Flowers from the past&lt;/h3&gt;
&lt;p&gt;Earlier this year, as I began a year-long personal sabbatical after my last two years as Engineering Manager overseeing the &lt;em&gt;&lt;a href=&quot;https://nymag.com/&quot;&gt;New York Magazine&lt;/a&gt;&lt;/em&gt; network of sites, I was eager to return to roots and build projects as an &lt;em&gt;individual contributor&lt;/em&gt; again. I began by developing &lt;a href=&quot;https://frogchor.us/&quot;&gt;Frog Chorus&lt;/a&gt;, a single page web-app that allows mobile devices to sing to each other as if they were frogs, relying on acoustic proximity to communicate with each other instead of network protocols. It’s a playful gesture towards an internet that centralizes human sensory experience, distanced from the mimetic primacy of text and image that facilitates the network effects of large scale platforms.&lt;/p&gt;
&lt;p&gt;Through this sabbatical period of soul-searching around my professional and personal relationship to technology, I’ve felt an urgency towards the poetic capacity of the internet, of the kinds of projects covered by &lt;a href=&quot;https://www.naiveweekly.com/&quot;&gt;Naive Weekly&lt;/a&gt;, &lt;a href=&quot;https://thehtml.review/&quot;&gt;The HTML Review&lt;/a&gt;, and &lt;a href=&quot;https://sfpc.io/&quot;&gt;The School for Poetic Computation&lt;/a&gt;. At the threshold of what feels like a new technological epoch, I felt it necessary to revisit the energy that had brought me to software, and the web, in the first place. Software, not as a service, but as sculpture.&lt;/p&gt;
&lt;p&gt;On a trip to The Metropolitan Museum of Art in March 2023, I was struck by a couple paintings tucked into an alcove by the French Symbolist painter Odilon Redon (whose painting is featured as the lede image of this essay). Although he is predominantly known for his work with charcoal, I was enthralled both by his delicate use of color, and by the exhibition text, which informed me that he wrote about his own work in the following way:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“All my originality, then, consists in giving human life to unlikely creatures according to the laws of probability, while, as much as possible, putting the logic of the visible at the service of the invisible” - &lt;a href=&quot;https://brooklynrail.org/2005/11/artseen/beyond-the-visible-the-art-of-odilon-red#:~:text=%E2%80%9CMy%20originality%2C%E2%80%9D%20wrote%20Redon%2C%20%E2%80%9Cconsists%20in%20bringing%20to%20life%E2%80%A6improbable%20beings%20and%20making%20them%20live%20according%20to%20the%20laws%20of%20probability%2C%20by%20putting%E2%80%94as%20far%20as%20possible%E2%80%94the%20logic%20of%20the%20Visible%20at%20the%20service%20of%20the%20Invisible.%E2%80%9D&quot;&gt;Odilon Redon&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With its invocation of &lt;em&gt;bringing to life&lt;/em&gt; and laws of &lt;em&gt;probability&lt;/em&gt;, this statement feels as appropriate for the aims of AI, as the work of a painter. But what exactly is this &lt;em&gt;invisible&lt;/em&gt; thing? Christian theology was likely not far from Redon’s mind, but I would speculate that the technologic society of today seems to be much more about routing the logic of the visible back into itself, a complete, but unfulfilling circle. Around the same time I visited Redon’s paintings, I had finished reading Iain McGilchrist’s &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/The_Master_and_His_Emissary&quot;&gt;The Master and His Emissary&lt;/a&gt;&lt;/em&gt;, a 600-page survey of Western civilization through the lens of lateralized brain function and structure (it turns out that sabbaticals are great for finishing books shaped like cement blocks). In his work, McGilchrist advocates for exactly what Redon describes, formalized instead as the analytic, logic-oriented left hemisphere of the brain serving not itself, but the right hemisphere of the brain, which integrates the logic of parts into the &lt;em&gt;whole&lt;/em&gt;.&lt;/p&gt;
&lt;!-- I would instead refer the reader to Iain McGilchrist&#39;s [The Master and His Emissary](https://en.wikipedia.org/wiki/The_Master_and_His_Emissary), a survey of Western civilzation through the lens of lateralized brain function and structure, in which he advocates for precisely what Redon describes. --&gt;
&lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt;
&lt;p&gt;Both visibly and invisibly, I’m pleased to share &lt;a href=&quot;https://reubenson.com/midi-archive/&quot;&gt;the informal archive and machine learning model&lt;/a&gt; I’ve developed during this residency.&lt;/p&gt;
&lt;p&gt;The 800k parameters decoder-only transformer model has been deployed as an AWS Lambda function, which is triggered every day around noon (GMT) to produce a few minutes of music. Each time the model is invoked, it uses tokens generated from the previous execution to produce subsequent tokens, and in this way produces a single continuous piece of music that has no end, but is punctuated by a 24-hour cycle of rest. My intent is to have the model be able to express something at once whimsical and general about the underlying archive, and serve as an entrypoint to education regarding machine learning and music on the early internet. It does not represent the state of the art in 2023, nor is it intended to be used seriously as a tool for music creation. Furthermore,the collection of files comprising the archive is small (~3000 files), and represents a diverse range of sources (Bach, pop, prog, video game soundtracks, early and medieval music, etc), such that the model is unable to develop a coherent understanding of what music &lt;em&gt;actually&lt;/em&gt; is.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“I listened to the music. It was hideous. I have never heard anything like it. It was distorted, diabolical, without sense or meaning, except, perhaps, an alien, disconcerting meaning that should never have been there. I could believe only with the greatest effort that it had once been a Bach Fugue, part of a most orderly and respected work.” - Philip K. Dick, &lt;a href=&quot;https://sickmyduck.narod.ru/pkd097-0.html&quot;&gt;The Preserving Machine&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As I was developing this project, I got an insightful suggestion from my friend &lt;a href=&quot;https://laurel.world/&quot;&gt;Laurel&lt;/a&gt; to check out Philip K Dick’s short story, &lt;em&gt;&lt;a href=&quot;https://sickmyduck.narod.ru/pkd097-0.html&quot;&gt;The Preserving Machine&lt;/a&gt;&lt;/em&gt;, in which living machines are produced to preserve music of the past, only to end up creating monstrosities only circumstantially connected to their original aims. I feel this rather neatly articulates the space I’ve been exploring between AI and archives, that our notions of preservation and generation are leaky abstractions. This passage also captures the spirit of the &lt;em&gt;“music”&lt;/em&gt; produced by the model, which has a furthermore uncanny quality owing to the use of the &lt;a href=&quot;https://www.midi.org/specifications-old/item/gm-level-1-sound-set&quot;&gt;General MIDI sound specification&lt;/a&gt; combined with the &lt;em&gt;free improvisation&lt;/em&gt; energy of something &lt;a href=&quot;https://youtu.be/ponl1XikN1Y?si=9v4ENW9uhFFtoV3-&amp;amp;t=254&quot;&gt;like a Zeena Parkins ensemble&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I may or may not decide to further develop this project, but I hope in its current prototypal state, it manages to convey something about what drives humans to develop, use, and share technology. For more details on the implementation, check out the flow diagram below, and repositories on GitHub at &lt;a href=&quot;https://github.com/reubenson/midi-archive&quot;&gt;https://github.com/reubenson/midi-archive&lt;/a&gt; and &lt;a href=&quot;https://github.com/reubenson/midi-archive-lambda&quot;&gt;https://github.com/reubenson/midi-archive-neural-net&lt;/a&gt;.&lt;/p&gt;
&lt;!-- Like in Philip K Dick&#39;s [Preserving Machine](https://sickmyduck.narod.ru/pkd097-0.html), the twin impulses to conserve and to generate seems to become indistinguishably intertwined. I may or may not decide to continue working on this project, but I hope in its current prototypal state, it manages to convey something about what drives humans to develop, use, and share technology. --&gt;
&lt;!-- The neural net model used here follows directly from Andrej Karpathy&#39;s [pedagogical model](https://karpathy.ai/zero-to-hero.html), and you can [check out and run the source code yourself on Colab](https://colab.research.google.com/drive/1hpzG6ygsn0Cv44ImhyOn13eHtSo_Lccg#scrollTo=2KCKQ2kVr24C). My intent is to have the model be able to express something at once whimsical and general about the underlying archive, and to be simple enough to serve educational purposes. It does not represent the state of the art in 2023, nor is it intended to be used seriously as a tool for music creation. Furthermore, I planned for this model to be portable and cheap to run.  --&gt;
&lt;figure&gt;
&lt;img src=&quot;https://mermaid.ink/img/pako:eNqVU02L2zAQ_SuDYEELm1NvORSSOMkG0m0hgS4ll4k8SURsKR3JSdPd_e8dyc4H7anG4BF682bmvfGbMr4k1Vebyp_MDjnCslg5kGegF4bxQPBlVsxgYysKsGFfAyFXZzjR-hF6vc_wQ39jCuQi1EJVgW-ivICuBOsiMZpojx0Lstmlg3cwrugoSWcIEaM1EGykx65y5h0KrzcUQpsqXB6i35OTNjxDZLTOui04ahgr-cQu_eEBhplh_t8MuevmUHksBQmLTzBszP7K3NKO9ISi2V2oUkoO7W_iDjjKwEIvU4l7_ixRByoyaKzHvw6eL-pJ1a8vL68dZJwhE12Q9HROl4PvC5hjvS7xNu0ko151gVaMGVW-KU-YO2S73V6bamHv_6CWLep9qqfkxK_YmRXoZ0POUJ4woNiWJflb5ULPk1w-qZvzBp3JsiF3pl7xMz1kfwrEYCqb1sZ458jEAEeLOcknyUOq1k6aIiZD1zVas1Q0GO4cn2buZ32bq9dNT-WFhgnLcFvndptloHaNb1RyfG5P08uVelI1cY22lH_lLV2uVNxRTSvVl7BE3q_Uyn0IDmX_F2dnVD9yQ0-qOZQiaWFxy1ir_garQB9_AMw-Gdc?type=png&quot; alt=&quot;drawing&quot; style=&quot;max-width:420px; margin: auto;&quot; /&gt;
&lt;figcaption&gt;Flow diagram for the project&lt;/figcaption&gt;
&lt;!-- source: https://mermaid.live/edit#pako:eNplUsFqGzEQ_ZVBJwXiQujNh4LttR1DCgUHmqSbw1ia3RXRStuR1ls35N8r7W5JQ3UaoffezLynV6G8JrEUlfWDapAj3Belg3RW8qgYO-IrWCy-PMnCD8561PD1UBygMpYCVOxbOLhI7CjCilVjznQ18Z8yDdbySHymiYQhUAxwNgh7E5v-9A3rpDKkGkLEaNQimEhQkyNOV--gQact6QlzcxMvs_p6VN_IHUXVwKffpgNf_Tta9NAHSi0hMhpnXA2pOVSewVHPaCGP3Kbt7Sy5yZLbH_cZ_h_mecJsM-ZRbn91nj-yH8eBdrKgzvpLbr_6foQ7bE8aZ8huhDzIAo29wMb6Xg-Yx49s6joZPcEeRthe7icXZu8C_ezJqbSR0xAwWZpaHD9_sKOQdzkg79LTSJoTgYFO2dgZXIzgg1yzHwIxKGvIRVDeOVJzPpnh1UuOK4lNa-SKSZH5m-eJUzuFIc7Ct_J9qcW8VMpuZjOhDu8RTZ8nbTBNMynsU31bOnEtWuIWjU5f8zU_lSI21FIplqnUyC-lKN1bwmEf_fHilFhG7ula9J1OnhUGa8ZWLCu0gd7-ANel7tU --&gt;
&lt;/figure&gt;
&lt;p&gt;And if you’re a technologist and any of this sparks inspiration to make time for realigning your relationship to software and technology, consider &lt;a href=&quot;https://www.recurse.com/apply&quot;&gt;applying to Recurse Center&lt;/a&gt; to participate in an upcoming batch!&lt;/p&gt;
&lt;!-- I feel that this story elegantly clarifies an uneasy sense of the porousness between AI and archives, that our notions of preservation and generation are only separated in abstraction. The term _generative AI_ gestures at this porousness too, as it immediately calls into question the derivative/conservative nature, not only of generative AI art, but human-made art as well. With all this in mind, I felt greater interest in using this project to explore the interdependence betweeen archives and AI, and how they each act as carriers for our own [_invisible_] fantasies of transformations and conservation.  --&gt;
&lt;!-- &lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt;
&lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt;
&lt;div class=&quot;divider-line&quot;&gt;〜〜〜&lt;/div&gt; --&gt;
&lt;!-- I developed this project during my residency at Recurse Center in the fall of 2023 as an exploration of the dynamics between building an archive and building a machine learning model. The media ecosystem that emerges from widespread usage of generative AI feels unknowable at this time, but looking back to the brief window of time when music on the web was dominated by MIDI may tell us something about what drives humans to develop, use, and share technology. In machine learning and the archive, there exist archetypes of transformation and conservation, containers for the hopes and fears about how we ourselves may change.

And with that, I&#39;m pleased to share 

How curiosities and anxieties 

, like that of the industry rapidly changing around me, and how I fit into this changing world. 

Like many members of my cohort, many of these questions were inflected by recent developments in AI, and so I decided to jump in and oriented my residency around learning ML fundamentals. I began with Andrej Karpathy&#39;s video series, going from building a simple neural net, on up to a transformer model (the subject of the landmark [Attention is All You Need]() paper). The emergent behavior from simple mechanics fascinated and perplexed me, but I also found myself caught between questions of what the purpose of these models is, and my own relationship to them.



- Left my job at the beginning of the year
- I arrived at RC after leaving my post as EM, and not having worked as an IC in over two years
- Wanting to give back to the internet things that I find beautiful and poetic
- Open questions about my role in the larger world of software
- Writing software as a tool for self-discovery

- Easier to fixate on what I didn’t do, vs what I did manage to do
- Taking the opportunity to work on software full-time again
- Taking a step back to see what draws me forward
- Anxiety about the job market, and thinking about switching gears back to IC
- How does one learn?
- What draws me to tech, and is software something I would be developing in the absence of work?
- What can AI teach me about building technology?
- Larger theme of transformation … 
- what are the interesting problems worth exploring solutions for
- The preserving machine
- A period of reconnecting to the underlying human impulses that manifest on the web, the kinds of projects surveyed by Naive Weekly on Substack --&gt;</content>
  </entry>
  
  <entry>
    <title>Website as a meandering home</title>
    <link href="https://reubenson.com/recurse/meander/"/>
    <updated>2025-05-28T07:46:24Z</updated>
    <id>https://reubenson.com/recurse/meander/</id>
    <content type="html">&lt;figure class=&quot;figure-medium&quot;&gt;
  &lt;img src=&quot;https://reubenson.com/public/meander-tile.jpg&quot; alt=&quot;photo of meander pattern in apartment building&quot; /&gt;
  &lt;figcaption&gt;The tile work in my apartment building, which has been reproduced as a border across my website&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;I first registered the domain, &lt;a href=&quot;http://reubenson.com/&quot;&gt;reubenson.com&lt;/a&gt;, on 01/20/2016. At the time, I was navigating various transitions, having moved from Boston to New York City, and was in the process of pivoting from research science to web development. I was living in the Ditmas Park neighborhood, and I spent a lot of time walking along the lake in the southern portion of Prospect Park. I lived directly below the cartoonist &lt;a href=&quot;https://en.wikipedia.org/wiki/Gary_Panter&quot;&gt;Gary Panter&lt;/a&gt;, and on several occasions, I improvised duets with him, unbeknownst to Gary, in which I could hear the sound of him playing guitar through my ceiling (his floor) and I joined with my guitar. The website I launched then evolved slightly from 2016 to 2023, and you can visit it on &lt;a href=&quot;https://web.archive.org/web/20230406031235/https://reubenson.com/&quot;&gt;archive.org here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The version of the website you’re visiting now began to take shape in the fall of 2023, just as I was beginning a residency at &lt;a href=&quot;https://recurse.com/&quot;&gt;Recurse Center&lt;/a&gt;. I was also beginning to wrap up my year-long personal sabbatical, and trying to understand the shape of my life to come. New chapters of life sometimes need new websites.&lt;/p&gt;
&lt;p&gt;This website is still under construction, as I find my (pathless) path, but I’ve really been appreciating the framework of “personal website as private engine of change”. I’ve never really felt at home online, and maybe for this reason, I’ve found it compelling to incorporate an architectural design motif that is repeated across my current apartment building into the borders of the pages herein. I’ve &lt;a href=&quot;https://medium.com/@reubenson/foray-into-3d-printing-with-clay-at-haystack-207064511cd&quot;&gt;written elsewhere a little&lt;/a&gt; about my interest in the Greek meander pattern, and while the border used here isn’t exactly a meander pattern, it bears more than a passing resemblance.&lt;/p&gt;
&lt;p&gt;In my continued meander through life, I’m glad to have a website that meanders with me.&lt;/p&gt;
</content>
  </entry>
</feed>